{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "52d6ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor flow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensor flow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6a582",
   "metadata": {},
   "source": [
    "## 1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be40b8",
   "metadata": {},
   "source": [
    "### Descargar el conjunto de datos de rótulos de tráfico que contiene las imágenes de las 43 clases mencionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4c0541cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['coords', 'labels', 'features', 'sizes'])\n",
      "Key: coords, Type: <class 'numpy.ndarray'>, Length: 34799\n",
      "Key: labels, Type: <class 'numpy.ndarray'>, Length: 34799\n",
      "Key: features, Type: <class 'numpy.ndarray'>, Length: 34799\n",
      "Key: sizes, Type: <class 'numpy.ndarray'>, Length: 34799\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo .p\n",
    "with open('data/Datos_Rotulos_Trafico/entrenamiento.p', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Imprimir las claves disponibles\n",
    "print(data.keys())\n",
    "\n",
    "# Inspeccionar un poco más el contenido\n",
    "for key in data.keys():\n",
    "    print(f\"Key: {key}, Type: {type(data[key])}, Length: {len(data[key])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33807f4",
   "metadata": {},
   "source": [
    "### Dividir el conjunto de datos en conjuntos de entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9c8fd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar los datos desde un archivo .p\n",
    "def load_data(pickle_file):\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data['features'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5534fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los conjuntos de datos\n",
    "X_train, y_train = load_data('data/Datos_Rotulos_Trafico/entrenamiento.p')\n",
    "X_val, y_val = load_data('data/Datos_Rotulos_Trafico/validacion.p')\n",
    "X_test, y_test = load_data('data/Datos_Rotulos_Trafico/prueba.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fffa45",
   "metadata": {},
   "source": [
    "\n",
    "### Realizar preprocesamiento de las imágenes, como redimensionarlas a un tamaño estándar, normalización, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c32b70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convertir etiquetas a arrays de NumPy (aunque ya están en ese formato, es bueno asegurarse)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = tf.image.rgb_to_grayscale(X_train)\n",
    "X_val = tf.image.rgb_to_grayscale(X_val)\n",
    "X_test = tf.image.rgb_to_grayscale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5faba4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (34799, 32, 32, 1), Labels shape: (34799,)\n",
      "Validation data shape: (4410, 32, 32, 1), Labels shape: (4410,)\n",
      "Test data shape: (12630, 32, 32, 1), Labels shape: (12630,)\n"
     ]
    }
   ],
   "source": [
    "# Verificar la forma de los datos\n",
    "print(f'Train data shape: {X_train.shape}, Labels shape: {y_train.shape}')\n",
    "print(f'Validation data shape: {X_val.shape}, Labels shape: {y_val.shape}')\n",
    "print(f'Test data shape: {X_test.shape}, Labels shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98bfab",
   "metadata": {},
   "source": [
    "## 2. Implementación de la arquitectura Le-Net:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e69569",
   "metadata": {},
   "source": [
    "### Presentar la arquitectura Le-Net en detalle, explicando cada capa (convolucional, pooling, fully connected)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca3c17",
   "metadata": {},
   "source": [
    "LeNet es una de las primeras arquitecturas de redes neuronales convolucionales, desarrollada por Yann LeCun y su equipo en 1998. Fue diseñada inicialmente para reconocer dígitos escritos a mano en imágenes de 28x28 píxeles, como los dígitos del conjunto de datos MNIST. A continuación, se detalla la arquitectura de LeNet:\n",
    "\n",
    "#### 1. Capa Convolucional 1 (Conv1)\n",
    "- **Entrada**: Una imagen de 32x32 píxeles (en blanco y negro, con un solo canal).\n",
    "- **Operación**: Se aplica un filtro convolucional de tamaño 5x5, con 6 filtros en total, produciendo 6 mapas de características.\n",
    "- **Salida**: 6 mapas de características, cada uno de tamaño 28x28 píxeles.\n",
    "\n",
    "#### 2. Capa de Submuestreo (Pooling) 1 (Pool1)\n",
    "- **Entrada**: Los 6 mapas de características de la capa anterior.\n",
    "- **Operación**: Se aplica un max-pooling (o submuestreo) con un tamaño de ventana 2x2 y un stride de 2, lo que reduce las dimensiones de cada mapa de características.\n",
    "- **Salida**: 6 mapas de características de tamaño 14x14 píxeles.\n",
    "\n",
    "#### 3. Capa Convolucional 2 (Conv2)\n",
    "- **Entrada**: Los 6 mapas de características de la capa anterior.\n",
    "- **Operación**: Se aplican 16 filtros convolucionales de tamaño 5x5, produciendo 16 nuevos mapas de características.\n",
    "- **Salida**: 16 mapas de características, cada uno de tamaño 10x10 píxeles.\n",
    "\n",
    "#### 4. Capa de Submuestreo (Pooling) 2 (Pool2)\n",
    "- **Entrada**: Los 16 mapas de características de la capa anterior.\n",
    "- **Operación**: Se aplica max-pooling con una ventana de 2x2 y un stride de 2.\n",
    "- **Salida**: 16 mapas de características de tamaño 5x5 píxeles.\n",
    "\n",
    "#### 5. Capa Fully Connected (FC1)\n",
    "- **Entrada**: Las salidas aplanadas de los 16 mapas de características de tamaño 5x5, lo que da un vector de 400 elementos (16x5x5).\n",
    "- **Operación**: Se conecta cada una de las 400 entradas a 120 neuronas completamente conectadas.\n",
    "- **Salida**: Un vector de 120 elementos.\n",
    "\n",
    "#### 6. Capa Fully Connected (FC2)\n",
    "- **Entrada**: El vector de 120 elementos de la capa anterior.\n",
    "- **Operación**: Se conecta cada uno de los 120 valores a 84 neuronas completamente conectadas.\n",
    "- **Salida**: Un vector de 84 elementos.\n",
    "\n",
    "#### 7. Capa de Salida (Output Layer)\n",
    "- **Entrada**: El vector de 84 elementos.\n",
    "- **Operación**: Se conecta a una capa de salida con tantas neuronas como clases de salida, en este caso 10 (para los dígitos del 0 al 9 en MNIST).\n",
    "- **Salida**: Un vector de 43 elementos\n",
    "\n",
    "En resumen, la arquitectura LeNet es un ejemplo clásico de una red neuronal convolucional, que combina capas convolucionales para extracción de características y capas completamente conectadas para la clasificación. Aunque sencilla en comparación con arquitecturas modernas, LeNet sentó las bases para muchos avances en el campo de la visión por computadora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b8a90",
   "metadata": {},
   "source": [
    "### Mostrar el diseño de la red Le-Net utilizando una herramienta de diagramación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1326a5e",
   "metadata": {},
   "source": [
    "![Diagrama en blanco (1).png](<attachment:Diagrama en blanco (1).png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051fa48f",
   "metadata": {},
   "source": [
    "La red LeNet toma como entrada una imagen de 32x32 píxeles y la procesa a través de varias capas para extraer características y clasificarlas. El flujo de la red es el siguiente:\n",
    "\n",
    "1. **Input Layer**: Recibe la imagen de entrada de 32x32 píxeles con un solo canal (en blanco y negro).\n",
    "\n",
    "2. **Conv1 Layer**: Aplica 6 filtros convolucionales de tamaño 5x5 para generar 6 mapas de características, cada uno de 28x28 píxeles.\n",
    "\n",
    "3. **Pool1 Layer**: Realiza una operación de submuestreo (pooling) para reducir el tamaño de los mapas de características a 14x14 píxeles.\n",
    "\n",
    "4. **Conv2 Layer**: Aplica 16 filtros convolucionales de tamaño 5x5, produciendo 16 mapas de características de 10x10 píxeles.\n",
    "\n",
    "5. **Pool2 Layer**: Aplica otro submuestreo para reducir el tamaño de los mapas de características a 5x5 píxeles.\n",
    "\n",
    "6. **FC1 Layer**: Aplana las características y las pasa a través de una capa completamente conectada con 120 neuronas.\n",
    "\n",
    "7. **FC2 Layer**: Las salidas de la capa anterior se pasan a otra capa completamente conectada con 84 neuronas.\n",
    "\n",
    "8. **Output Layer**: Finalmente, la capa de salida consiste en 10 neuronas, que corresponden a las 10 clases posibles para la clasificación.\n",
    "\n",
    "Este flujo de procesamiento permite que la red LeNet extraiga y combine características a diferentes niveles de abstracción, culminando en la predicción de la clase de la imagen de entrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01be13",
   "metadata": {},
   "source": [
    "### Explicar el proceso de convolución, función de activación y pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ace67",
   "metadata": {},
   "source": [
    "**Proceso de Convolución**\n",
    "\n",
    "El proceso de convolución es una operación fundamental en las redes neuronales convolucionales (CNNs) que consiste en aplicar un filtro (o kernel) sobre la entrada (como una imagen) para extraer características importantes. El filtro se desliza (o convoluciona) sobre la entrada, realizando multiplicaciones punto a punto entre los valores del filtro y las secciones correspondientes de la imagen. Luego, se suman los productos resultantes para obtener un solo valor que representa la activación en esa ubicación específica. Este proceso se repite para cada posición del filtro, produciendo un mapa de características que resalta las características detectadas por el filtro en la imagen original.\n",
    "\n",
    "**Función de Activación**\n",
    "\n",
    "Una vez que la convolución se ha aplicado, el resultado suele pasar a través de una función de activación, siendo la más común la función ReLU (Rectified Linear Unit). La función ReLU se define como \\( f(x) = \\max(0, x) \\), es decir, reemplaza todos los valores negativos por cero y deja los valores positivos sin cambios. Esta no linealidad permite a la red aprender una amplia gama de patrones complejos y aumentar su capacidad de representación. La función de activación se aplica a cada valor en el mapa de características generado por la convolución.\n",
    "\n",
    "**Pooling**\n",
    "\n",
    "El pooling es una operación de reducción de dimensionalidad que se aplica sobre los mapas de características después de la convolución y la activación. La operación más común es el Max Pooling, donde se selecciona el valor máximo dentro de una ventana (por ejemplo, de 2x2) que se desliza sobre el mapa de características. El pooling reduce el tamaño del mapa de características, haciendo que la red sea más computacionalmente eficiente y menos propensa a sobreajustarse. También ayuda a que la red sea invariante a pequeñas traslaciones de la entrada, mejorando la capacidad de generalización del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393db86",
   "metadata": {},
   "source": [
    "## 3. Construcción del modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5d1c4",
   "metadata": {},
   "source": [
    "### Utilizar la biblioteca de aprendizaje profundo TensorFlow para construir la arquitectura Le-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c515c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12d0f2",
   "metadata": {},
   "source": [
    "### Definir la estructura de capas convolucionales, capas de pooling y capas fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "632eee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa de convolución 1\n",
    "model.add(layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "# Capa de convolución 2\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "# Aplanar las salidas\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Capa completamente conectada 1\n",
    "model.add(layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "# Capa completamente conectada 2\n",
    "model.add(layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(layers.Dense(units=43, activation='softmax'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9c7b604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_27 (Conv2D)          (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d_6 (Averag  (None, 14, 14, 6)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_7 (Averag  (None, 5, 5, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 400)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 43)                3655      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64,511\n",
      "Trainable params: 64,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0f726",
   "metadata": {},
   "source": [
    "### Explicar la importancia de la función de pérdida y el optimizador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6d521",
   "metadata": {},
   "source": [
    "**Función de Pérdida**\n",
    "\n",
    "La función de pérdida es un componente crucial en el entrenamiento de una red neuronal. Su principal objetivo es cuantificar cuán lejos están las predicciones del modelo de las verdaderas etiquetas. En otras palabras, mide el \"costo\" o \"error\" asociado a las predicciones actuales de la red. Durante el entrenamiento, el modelo intenta minimizar esta función de pérdida, ajustando los pesos de la red para que las predicciones se acerquen lo más posible a las etiquetas correctas. Existen diferentes tipos de funciones de pérdida, como la entropía cruzada para tareas de clasificación o el error cuadrático medio para tareas de regresión, cada una adecuada para diferentes tipos de problemas.\n",
    "\n",
    "**Optimizador**\n",
    "\n",
    "El optimizador es el algoritmo que ajusta los pesos y sesgos de la red neuronal en función del valor de la función de pérdida. Su papel es minimizar la función de pérdida mediante un proceso iterativo de ajuste de los parámetros del modelo. Uno de los optimizadores más comunes es el Descenso de Gradiente Estocástico (SGD), que ajusta los parámetros en la dirección opuesta al gradiente de la función de pérdida con respecto a los parámetros. Variantes más avanzadas como Adam, RMSprop, y AdaGrad, entre otros, incluyen mejoras para manejar la velocidad de aprendizaje de manera adaptativa y reducir la variabilidad del descenso de gradiente, acelerando así la convergencia y mejorando la estabilidad del entrenamiento.\n",
    "\n",
    "**Importancia Conjunta**\n",
    "\n",
    "La combinación de una función de pérdida y un optimizador es fundamental para el entrenamiento efectivo de una red neuronal. Mientras que la función de pérdida proporciona una medida del error que el modelo debe minimizar, el optimizador determina cómo el modelo debe actualizar sus pesos para reducir ese error. Juntos, estos componentes permiten que la red aprenda patrones complejos en los datos, mejorando su capacidad para hacer predicciones precisas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9983e74",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "El proceso de entrenamiento de la red neuronal comienza con la carga y normalización de los datos, escalando las imágenes al rango de [0, 1] para mejorar la eficiencia. La red, se basa en la arquitectura LeNet, incluye capas convolucionales y de pooling para extraer características y reducir la dimensionalidad, seguidas por capas densas que realizan la clasificación final. Durante el entrenamiento, se utiliza un optimizador para ajustar los pesos de la red mediante mediante la minimización de una función de pérdida, mediante retropropagación. Los hiperparámetros como la tasa de aprendizaje, el número de épocas y el tamaño de los lotes se configuran con el fin de optimizar el rendimiento del modelo, que se entrena y valida iterativamente para garantizar su capacidad de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e6f1246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9b8c424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8da47287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "272/272 [==============================] - 14s 49ms/step - loss: 2.2790 - accuracy: 0.3989 - val_loss: 1.2583 - val_accuracy: 0.6685\n",
      "Epoch 2/20\n",
      "272/272 [==============================] - 13s 49ms/step - loss: 0.7888 - accuracy: 0.7828 - val_loss: 0.8737 - val_accuracy: 0.7610\n",
      "Epoch 3/20\n",
      "272/272 [==============================] - 13s 48ms/step - loss: 0.4947 - accuracy: 0.8689 - val_loss: 0.7342 - val_accuracy: 0.8141\n",
      "Epoch 4/20\n",
      "272/272 [==============================] - 13s 49ms/step - loss: 0.3636 - accuracy: 0.9028 - val_loss: 0.6306 - val_accuracy: 0.8338\n",
      "Epoch 5/20\n",
      "272/272 [==============================] - 14s 51ms/step - loss: 0.2831 - accuracy: 0.9261 - val_loss: 0.5676 - val_accuracy: 0.8476\n",
      "Epoch 6/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.2264 - accuracy: 0.9404 - val_loss: 0.6015 - val_accuracy: 0.8438\n",
      "Epoch 7/20\n",
      "272/272 [==============================] - 13s 46ms/step - loss: 0.1821 - accuracy: 0.9526 - val_loss: 0.5544 - val_accuracy: 0.8628\n",
      "Epoch 8/20\n",
      "272/272 [==============================] - 13s 46ms/step - loss: 0.1536 - accuracy: 0.9606 - val_loss: 0.5745 - val_accuracy: 0.8653\n",
      "Epoch 9/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.1291 - accuracy: 0.9669 - val_loss: 0.5721 - val_accuracy: 0.8753\n",
      "Epoch 10/20\n",
      "272/272 [==============================] - 13s 46ms/step - loss: 0.1096 - accuracy: 0.9719 - val_loss: 0.5873 - val_accuracy: 0.8751\n",
      "Epoch 11/20\n",
      "272/272 [==============================] - 13s 48ms/step - loss: 0.0964 - accuracy: 0.9745 - val_loss: 0.5619 - val_accuracy: 0.8780\n",
      "Epoch 12/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.0878 - accuracy: 0.9765 - val_loss: 0.5794 - val_accuracy: 0.8780\n",
      "Epoch 13/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.0714 - accuracy: 0.9811 - val_loss: 0.6028 - val_accuracy: 0.8798\n",
      "Epoch 14/20\n",
      "272/272 [==============================] - 12s 46ms/step - loss: 0.0635 - accuracy: 0.9825 - val_loss: 0.6801 - val_accuracy: 0.8748\n",
      "Epoch 15/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.0569 - accuracy: 0.9841 - val_loss: 0.7671 - val_accuracy: 0.8800\n",
      "Epoch 16/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.0522 - accuracy: 0.9857 - val_loss: 0.6699 - val_accuracy: 0.8800\n",
      "Epoch 17/20\n",
      "272/272 [==============================] - 13s 46ms/step - loss: 0.0462 - accuracy: 0.9863 - val_loss: 0.6224 - val_accuracy: 0.8878\n",
      "Epoch 18/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.0418 - accuracy: 0.9876 - val_loss: 0.6864 - val_accuracy: 0.8859\n",
      "Epoch 19/20\n",
      "272/272 [==============================] - 13s 49ms/step - loss: 0.0446 - accuracy: 0.9878 - val_loss: 0.7746 - val_accuracy: 0.8707\n",
      "Epoch 20/20\n",
      "272/272 [==============================] - 13s 47ms/step - loss: 0.0335 - accuracy: 0.9903 - val_loss: 0.6837 - val_accuracy: 0.8939\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, \n",
    "                    validation_data=validation_dataset, \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98e450",
   "metadata": {},
   "source": [
    "# Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "778e44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 1s 1ms/step\n",
      "Precisión: 0.89\n",
      "F1-Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f'Precisión: {precision:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "43d4b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 1s 1ms/step\n",
      "Matriz de Confusión:\n",
      "[[ 30  23   0 ...   0   0   0]\n",
      " [  3 645  60 ...   0   0   0]\n",
      " [  0  20 698 ...   0   0   0]\n",
      " ...\n",
      " [  0   1   0 ...  73   0   0]\n",
      " [  0   0   0 ...   0  49   0]\n",
      " [  0   0   0 ...   0   1  89]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.50      0.59        60\n",
      "           1       0.83      0.90      0.86       720\n",
      "           2       0.85      0.93      0.89       750\n",
      "           3       0.87      0.94      0.90       450\n",
      "           4       0.91      0.87      0.89       660\n",
      "           5       0.95      0.85      0.90       630\n",
      "           6       0.98      0.79      0.87       150\n",
      "           7       0.91      0.85      0.88       450\n",
      "           8       0.86      0.88      0.87       450\n",
      "           9       0.91      0.96      0.93       480\n",
      "          10       0.96      0.97      0.96       660\n",
      "          11       0.89      0.87      0.88       420\n",
      "          12       0.94      0.96      0.95       690\n",
      "          13       0.96      0.97      0.97       720\n",
      "          14       0.91      0.89      0.90       270\n",
      "          15       0.88      0.98      0.93       210\n",
      "          16       0.99      0.97      0.98       150\n",
      "          17       0.99      0.87      0.92       360\n",
      "          18       0.83      0.74      0.78       390\n",
      "          19       0.75      0.90      0.82        60\n",
      "          20       0.74      0.83      0.78        90\n",
      "          21       0.86      0.56      0.68        90\n",
      "          22       0.74      0.75      0.74       120\n",
      "          23       0.88      0.70      0.78       150\n",
      "          24       0.55      0.62      0.59        90\n",
      "          25       0.95      0.84      0.89       480\n",
      "          26       0.54      0.81      0.65       180\n",
      "          27       0.68      0.50      0.58        60\n",
      "          28       0.84      0.86      0.85       150\n",
      "          29       0.78      0.94      0.85        90\n",
      "          30       0.57      0.55      0.56       150\n",
      "          31       0.90      0.91      0.90       270\n",
      "          32       0.54      0.83      0.65        60\n",
      "          33       0.83      0.96      0.89       210\n",
      "          34       0.87      0.98      0.93       120\n",
      "          35       0.96      0.91      0.93       390\n",
      "          36       0.99      0.97      0.98       120\n",
      "          37       0.94      0.85      0.89        60\n",
      "          38       0.94      0.93      0.93       690\n",
      "          39       0.92      0.86      0.89        90\n",
      "          40       0.97      0.81      0.88        90\n",
      "          41       0.86      0.82      0.84        60\n",
      "          42       0.97      0.99      0.98        90\n",
      "\n",
      "    accuracy                           0.89     12630\n",
      "   macro avg       0.85      0.85      0.84     12630\n",
      "weighted avg       0.89      0.89      0.89     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Hacer predicciones con el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convertir las predicciones a clases\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Generar el reporte de clasificación\n",
    "class_report = classification_report(y_test, y_pred_classes)\n",
    "\n",
    "# Presentar los resultados\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfdbe3",
   "metadata": {},
   "source": [
    "## Interpretación\n",
    "\n",
    "Como se puede observar la precisión es considerabelemente alta para la aplicación dada, reconocimiento de señales de tránsito (sin embargo siempre es necesario evaluar con mayor precisión para justificar esta métrica), esto significa que el 88% de las predicciones del modelo son correctas.\n",
    "Mientras que F1-Score, con un 0.87, indica que el modelo tiene un buen equilibrio entre precisión y exhaustividad, significando que no solamente es capaz de predecir correctamente los casos positivos, sion que también detecta la mayoria de estos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9630043",
   "metadata": {},
   "source": [
    "El desempeño varía significativamente entre las clases. Por ejemplo, la clase 0 tiene una precisión y F1-score bajos (0.70 y 0.47, respectivamente), indicando que el modelo tiene dificultades para predecir correctamente esta clase. En contraste, clases como la 10 y la 13 tienen una precisión y F1-score muy altos (0.95 y 0.97, respectivamente), lo que sugiere que el modelo predice estas clases con alta precisión y consistencia. Las métricas de recall también varían, con algunas clases como la 6 y la 19 mostrando un recall bajo, lo que indica que el modelo no está capturando todas las instancias de estas clases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
